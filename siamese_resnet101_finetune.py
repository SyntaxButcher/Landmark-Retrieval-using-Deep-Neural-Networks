# -*- coding: utf-8 -*-
"""Siamese_ResNet101_valft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MdEyfbqB_XMFqF35UEksjF7MAXmTdk6F
"""

import tensorflow as tf
from tensorflow.keras.applications import ResNet101V2
from tensorflow.keras import layers, Model
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import Sequence
from tensorflow.keras.callbacks import ModelCheckpoint

import pandas as pd
import numpy as np
import time
from tqdm import tqdm
from itertools import combinations
import random
import cv2
from sklearn.utils import shuffle
import os
import pathlib
import pickle

# from google.colab import files
# files.upload()

def Subnet(input_shape, weights):
  backbone_model = ResNet101V2(input_shape=input_shape, include_top=False, weights=weights)

  # for layer in backbone_model.layers:
  #   layer.trainable=False

  conv2_x_out = backbone_model.get_layer('conv2_block3_out').output
  conv3_x_out = backbone_model.get_layer('conv3_block4_out').output
  conv4_x_out = backbone_model.get_layer('conv4_block23_out').output
  backbone_out = backbone_model.output

  # print(conv2_x_out.shape, conv3_x_out.shape,conv4_x_out.shape, backbone_out.shape)

  GAP_out = tf.keras.layers.GlobalAveragePooling2D()(backbone_out)

  dense1_out = tf.keras.layers.Dense(1024, activation=tf.nn.relu, name='dense_1')(GAP_out)
  dense2_out = tf.keras.layers.Dense(512, activation=tf.nn.relu, name='dense_2')(dense1_out)
  dense3_out = tf.keras.layers.Dense(256, activation=tf.nn.relu, name='dense_3')(dense2_out)

  dense1_exp_out = tf.expand_dims(dense1_out, 1)
  dense2_exp_out = tf.expand_dims(dense2_out, 1)
  dense3_exp_out = tf.expand_dims(dense3_out, 1)

  # print(dense1_exp_out.shape,dense2_exp_out.shape,dense3_exp_out.shape)

  conv2_x_resh_out = tf.reshape(conv2_x_out, (-1, conv2_x_out.shape[1]*conv2_x_out.shape[2], conv2_x_out.shape[3]))
  conv3_x_resh_out = tf.reshape(conv3_x_out, (-1, conv3_x_out.shape[1]*conv3_x_out.shape[2], conv3_x_out.shape[3]))
  conv4_x_resh_out = tf.reshape(conv4_x_out, (-1, conv4_x_out.shape[1]*conv4_x_out.shape[2], conv4_x_out.shape[3]))

  # print(conv2_x_resh_out.shape, conv3_x_resh_out.shape, conv4_x_resh_out.shape)

  attn1_out = tf.keras.layers.Attention()([dense3_exp_out, conv2_x_resh_out])
  attn2_out = tf.keras.layers.Attention()([dense2_exp_out, conv3_x_resh_out])
  attn3_out = tf.keras.layers.Attention()([dense1_exp_out, conv4_x_resh_out])

  # print(attn1_out.shape, attn2_out.shape, attn3_out.shape)

  concat_out = tf.concat([tf.squeeze(attn1_out, axis=1), tf.squeeze(attn2_out, axis=1), tf.squeeze(attn3_out, axis=1)], axis=1)
  # print(concat_out.shape)

  subnet_model = Model(backbone_model.input, concat_out)


  return subnet_model

def contrastive_loss(y_true, pred_dist, margin=1.0):
  loss = K.mean(y_true * K.square(pred_dist) + (1 - y_true) * K.square(K.maximum(margin - pred_dist, 0)))
  return loss

def contrastive_accuracy(y_true, pred_dist):
  accuracy = K.mean(K.equal(y_true, K.cast(pred_dist < 0.5, y_true.dtype)))
  return accuracy

def pair_generator(df, tqdm_desc=None, data_type='train'):
    landmark_groups = df.groupby('landmark_id')
    
    pair_list = []
    single_list = []
    
    if tqdm_desc == None:
      tqdm_desc = "Generating pairs"
    total_classes = len(landmark_groups)
    
    # Iterate through each landmark group and create similar pairs within the group
    for _, group in tqdm(landmark_groups, desc=tqdm_desc, total=total_classes):
        ids = group['id'].tolist()
        random.shuffle(ids)
        single_list.append(ids[0])
        if len(ids) > 1:
          if len(ids) % 2 != 0:
            ids.remove(ids[0])     
          for i in range(0, len(ids), 2):
            pair_list.append({"id1": ids[i], "id2": ids[i+1], "similarity": 1})

    if len(single_list) % 2 != 0:
      random_idx = random.randint(0, len(single_list)-2)
      single_list.append(single_list[random_idx])

    dissimilar_pairs = []

    if data_type == 'train':
      num_iter = 5
    else:
      num_iter = 1

    for i in tqdm(range(num_iter), desc=tqdm_desc):
      random.shuffle(single_list)
      for j in range(0, len(single_list)-1):
        dissimilar_pairs.append({"id1": single_list[j], "id2": single_list[j+1], "similarity": 0})

    pair_list.extend(dissimilar_pairs)
    random.shuffle(pair_list)
    pair_df = pd.DataFrame(pair_list)
    return pair_df

def Siamese(input_shape, weights):

  # Define the two input layers
  input_1 = tf.keras.layers.Input(shape=input_shape)
  input_2 = tf.keras.layers.Input(shape=input_shape)
  
  # Define the Subnet model
  subnet = Subnet(input_shape, weights)

  # Get the output of the Subnet model for the two input layers
  output_1 = subnet(input_1)
  output_2 = subnet(input_2)

  # Define the L1 distance layer
  #dist_layer = tf.keras.layers.Lambda(lambda x: K.sum(K.abs(x[0] - x[1]), axis=-1, keepdims=False))
  dist_layer = tf.keras.layers.Lambda(lambda x: K.sqrt(K.sum(K.square(x[0] - x[1]), axis=-1, keepdims=False)))
  distance = dist_layer([output_1, output_2])

  # Define the Siamese model
  siamese_model = Model(inputs=[input_1, input_2], outputs=distance)
  
  # Compile the Siamese model
  siamese_model.compile(loss=contrastive_loss, optimizer='adam', metrics=[contrastive_accuracy])
  
  return siamese_model

def process_image(file_path):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [300, 300])
    img /= 255.0  # Normalize pixel values to [0,1]
    return img

def load_image_pair_and_label(row):
    data_dir = './kaggle_datasets/train_dir/'
    img1_path = str(data_dir) + '/' + row['id1']
    img2_path = str(data_dir) + '/' + row['id2']
    img1 = process_image(img1_path)
    img2 = process_image(img2_path)
    label = tf.cast(row['similarity'], tf.float32)
    return (img1, img2), label

def split_df(df, num_splits):
    return np.array_split(df, num_splits)

def random_val_sample(val_dataset, sample_size, buffer_size):
    val_sample = val_dataset.shuffle(buffer_size).take(sample_size).batch(sample_size)
    return val_sample

class Valid_Saveparams(tf.keras.callbacks.Callback):
    def __init__(self, val_dataset, sample_size=256, accuracy_delta=0.0001, val_interval=100, buffer_size=1000):
        self.val_dataset = val_dataset
        self.sample_size = sample_size
        self.accuracy_delta = accuracy_delta
        self.validation_interval = val_interval
        self.buffer_size =  buffer_size
        super().__init__()

    def on_train_begin(self, logs=None):
        self.batch_losses = []
        self.batch_accs = []
        self.val_batch_accs = []

    def on_train_batch_end(self, batch, logs=None):
        self.batch_losses.append(logs['loss'])
        self.batch_accs.append(logs['contrastive_accuracy'])

        if batch % self.validation_interval == 0:
          val_sample = random_val_sample(self.val_dataset, self.sample_size, self.buffer_size)
          val_metrics = self.model.evaluate(val_sample, verbose=0)
          self.val_batch_accs.append(val_metrics[1])
          print(f' Validation accuracy: {val_metrics[1]}')

        if len(self.val_batch_accs) > 1 and abs(self.val_batch_accs[-1] - self.val_batch_accs[-2]) < self.accuracy_delta:
            print(f"Change in validation accuracy is below {self.accuracy_delta * 100:.2f}%, stopping training.")
            self.model.stop_training = True

input_shape = (300, 300, 3)
train_batch_size, valid_batch_size = 256, 128
weights = 'imagenet'
target_size=input_shape[:2]
num_epochs=1
buffer_size=1000


image_dir = pathlib.Path('./kaggle_datasets/train_dir/')
train_csv_path = './kaggle_datasets/train_split.csv'
valid_csv_path = './kaggle_datasets/test_split.csv'

train_df = pd.read_csv(train_csv_path)
valid_df = pd.read_csv(valid_csv_path)

train_pairs_df = pair_generator(train_df, tqdm_desc="Generating Train pairs", data_type='train')
valid_pairs_df = pair_generator(valid_df, tqdm_desc="Generating Valid pairs", data_type='valid')

train_dataset = tf.data.Dataset.from_tensor_slices(dict(train_pairs_df))
val_dataset = tf.data.Dataset.from_tensor_slices(dict(valid_pairs_df))

train_dataset = train_dataset.map(load_image_pair_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)
val_dataset = val_dataset.map(load_image_pair_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)

train_ds = train_dataset.shuffle(buffer_size).batch(train_batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
val_ds = val_dataset.batch(valid_batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

siamese_model = Siamese(input_shape, weights)
siamese_model.load_weights('siamese_weights_101_valcon2.h5')

callback = Valid_Saveparams(val_dataset)

siamese_model.fit(train_ds, epochs=num_epochs, callbacks=[callback])

siamese_model.save_weights("./model_weights/siamese_weights_101_valconft.h5")
print("Weights saved!")

with open('batch_history_101_valconft.pkl', 'wb') as f:
    pickle.dump({'loss': callback.batch_losses, 'accuracy': callback.batch_accs, 'val_accuracy': callback.val_batch_accs}, f)

# # Load the batch history data from the pickle file
# with open('batch_history.pkl', 'rb') as f:
#     loaded_data = pickle.load(f)

# loaded_losses = loaded_data['loss']
# loaded_accuracies = loaded_data['accuracy']

val_loss, val_contrastive_accuracy = siamese_model.evaluate(val_ds)

print(f"Validation loss: {val_loss:.4f}")
print(f"Validation contrastive accuracy: {val_contrastive_accuracy * 100:.2f}%")